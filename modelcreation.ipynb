{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wiggles\\miniconda3\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import random_split\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose, Resize\n",
    "import os\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels: {'g', 'b', 'M', 'C', 'm', 'Q', 'y', 'S', 'q', 'E', 'B', 'x', 'v', 't', 'P', 'I', '9', 'R', 'p', 'e', 'X', 'h', 'r', '2', 'Y', 'Z', 'V', '5', 's', '3', '7', 'a', 'K', 'd', '8', 'k', 'U', 'i', '1', 'H', 'n', 'O', 'J', 'A', 'W', 'G', '4', 'j', 'F', 'D', 'T', 'z', 'L', 'f', 'u', '6', 'c', 'w', 'N', 'l'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_all_labels(img_dir):\n",
    "    # List all files in the directory\n",
    "    filenames = os.listdir(img_dir)\n",
    "    \n",
    "    # Extract labels from filenames\n",
    "    labels = set()\n",
    "    for filename in filenames:\n",
    "        name = os.path.splitext(filename)[0]  # remove the extension\n",
    "        labels.update(list(name))  # add each character in the filename to the set of labels\n",
    "    \n",
    "    return labels\n",
    "\n",
    "img_dir = 'images'  # replace with your directory path\n",
    "all_labels = get_all_labels(img_dir)\n",
    "\n",
    "print(f\"Unique labels: {all_labels}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.img_names = os.listdir(img_dir)\n",
    "        self.transform = transform\n",
    "        self.label_mapping = {label: idx for idx, label in enumerate(all_labels)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "\n",
    "    def filename_to_labels(self, filename):\n",
    "        # Remove the file extension\n",
    "        name = os.path.splitext(filename)[0]\n",
    "        # Create a binary label vector for each character in the filename\n",
    "        labels = torch.zeros(len(self.label_mapping) * 5)\n",
    "        for char in name:\n",
    "            if char in self.label_mapping:\n",
    "                labels[self.label_mapping[char]*5 : (self.label_mapping[char]+1)*5] = 1\n",
    "        return labels\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_names[idx])\n",
    "        image = Image.open(img_path)\n",
    "\n",
    "        # Convert the filename to labels\n",
    "        labels = self.filename_to_labels(self.img_names[idx])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "            \n",
    "        return image, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose([\n",
    "    Resize((150,40)),  # Resize the images to 150x40\n",
    "    ToTensor(),  # Convert the image to a tensor\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomImageDataset('images', transform= transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_proportion = 0.8\n",
    "train_size = int(len(dataset) * train_proportion)\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # Input: 150x40x3\n",
    "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # After pool1: 75x20x64\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # After pool2: 37x10x128\n",
    "        self.fc1 = nn.Linear(37*10*128, 1024)  \n",
    "        self.fc2 = nn.Linear(1024, 512)  \n",
    "        self.fc3 = nn.Linear(512, 60*5)  # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1_1(x))\n",
    "        x = self.pool1(F.relu(self.conv1_2(x)))\n",
    "\n",
    "        x = self.pool2(F.relu(self.conv2_1(x)))\n",
    "\n",
    "        x = x.view(x.size(0), -1)  # flatten the tensor\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # we can apply a sigmoid here if we are using BCEWithLogitsLoss as the loss function\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        # Reshape the inputs to be 2D tensors with 5 columns\n",
    "        input = input.view(-1, 5)\n",
    "        target = target.view(-1, 5)\n",
    "\n",
    "        # Calculate the mean squared error for each 5-dimensional vector\n",
    "        mse = (input - target).pow(2).mean(dim=1)\n",
    "\n",
    "        # Return the average MSE over all vectors\n",
    "        return mse.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = CustomMSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "            # Convert the model's outputs into predictions\n",
    "            pred = torch.sigmoid(pred)  # Apply the sigmoid function to convert the scores into probabilities\n",
    "            pred = (pred > 0.5).float()  # Threshold the probabilities at 0.5 to get the predictions\n",
    "\n",
    "            # Calculate the number of correct predictions\n",
    "            correct += (pred == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size * 300  # Divide by the total number of labels\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.081694  [   64/90449]\n",
      "loss: 0.074096  [ 6464/90449]\n",
      "loss: 0.075192  [12864/90449]\n",
      "loss: 0.074049  [19264/90449]\n",
      "loss: 0.074302  [25664/90449]\n",
      "loss: 0.074539  [32064/90449]\n",
      "loss: 0.074719  [38464/90449]\n",
      "loss: 0.074638  [44864/90449]\n",
      "loss: 0.074398  [51264/90449]\n",
      "loss: 0.073975  [57664/90449]\n",
      "loss: 0.072890  [64064/90449]\n",
      "loss: 0.072880  [70464/90449]\n",
      "loss: 0.073369  [76864/90449]\n",
      "loss: 0.072812  [83264/90449]\n",
      "loss: 0.071818  [89664/90449]\n",
      "Test Error: \n",
      " Accuracy: 8.2%, Avg loss: 0.073330 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.072181  [   64/90449]\n",
      "loss: 0.072649  [ 6464/90449]\n",
      "loss: 0.073190  [12864/90449]\n",
      "loss: 0.072822  [19264/90449]\n",
      "loss: 0.074148  [25664/90449]\n",
      "loss: 0.072498  [32064/90449]\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_loader, model, loss_fn, optimizer)\n",
    "    test(test_loader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_disct(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Load and preprocess the image\u001b[39;00m\n\u001b[0;32m      2\u001b[0m img_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mimages\u001b[39m\u001b[39m'\u001b[39m  \n\u001b[1;32m----> 3\u001b[0m image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(img_path)\n\u001b[0;32m      4\u001b[0m image \u001b[39m=\u001b[39m transform(image)  \u001b[39m# apply the same transformations as during training\u001b[39;00m\n\u001b[0;32m      5\u001b[0m image \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)  \u001b[39m# add an extra dimension for the batch size\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the image\n",
    "img_path = 'images'  \n",
    "image = Image.open(img_path)\n",
    "image = transform(image)  # apply the same transformations as during training\n",
    "image = image.unsqueeze(0)  # add an extra dimension for the batch size\n",
    "image = image.to(device)  # move the image to the device where the model is\n",
    "\n",
    "# Make a prediction\n",
    "with torch.no_grad():  # disable gradient calculations\n",
    "    model.eval()  # put the model in evaluation mode\n",
    "    output = model(image)  # get the raw output\n",
    "\n",
    "# Postprocess the output\n",
    "prob = torch.sigmoid(output)  # convert the raw output into probabilities\n",
    "pred = (prob > 0.5).float()  # threshold the probabilities at 0.5 to get the predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
